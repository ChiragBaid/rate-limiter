# rate-limiter â€” High-Performance Token-Bucket Rate Limiter (Go)

**A production-minded, pluggable rate-limiter service implemented in Go.**  
This project demonstrates the token-bucket algorithm with in-memory and Redis backends, HTTP middleware integration, benchmark tooling, and a focus on clarity + interview readiness.

---

## ğŸš€ Features
- **Token-bucket algorithm** with configurable rate & burst capacity.
- **Pluggable backends**: in-memory (fast local) and Redis (distributed).
- **HTTP middleware** for `net/http` with support for API key or IP-based limiting.
- **Benchmarks included** (Vegeta + reports) with sample results.
- **CI-ready**: unit tests + GitHub Actions.

---

## ğŸ“‚ Repository Layout

<pre>
rate-limiter/
â”œâ”€ cmd/
â”‚  â””â”€ server/
â”‚     â””â”€ main.go
â”œâ”€ internal/
â”‚  â”œâ”€ limiter/
â”‚  â”‚  â”œâ”€ limiter.go
â”‚  â”‚  â””â”€ limiter_test.go
â”‚  â”œâ”€ backend/
â”‚  â”‚  â”œâ”€ memory.go
â”‚  â”‚  â””â”€ redis.go
â”‚  â””â”€ middleware/
â”‚     â””â”€ middleware.go
â”œâ”€ docs/
â”‚ â””â”€ diagram.png # architecture diagram
â”œâ”€ benchmarks/
â”‚ â”œâ”€ vegeta_attack.sh # vegeta load test script
â”‚ â”œâ”€ sample_report.csv # benchmark CSV 
â”‚ â””â”€ report_plot.png # benchmark graph
â”œâ”€ go.mod
â”œâ”€ README.md
â””â”€ .github/
   â””â”€ workflows/ci.yml
</pre>

---

## ğŸƒ Quick Start

### Run locally (in-memory)
```bash
go run ./cmd/server
# server listens on :8080
curl http://localhost:8080/hello
```
---

### Run with Redis
docker run -d -p 6379:6379 redis
REDIS_ADDR=localhost:6379 go run ./cmd/server

---
### Demo: Exceeding Rate Limit
```bash
curl -i http://localhost:8080/hello
```
# Eventually returns 429 Too Many Requests

---


## ğŸ“Š Benchmarks (Sample Results)
```markdown
Benchmarks were simulated using [Vegeta](https://github.com/tsenart/vegeta) with 10s runs on a local dev machine:

- **Machine:** Intel i7-9750H, 16GB RAM, SSD  
- **Go:** 1.21  
- **Redis:** 6.2 (Docker, single instance)  

| Backend   | Peak RPS | Avg Latency (p50) | p95 Latency | Error Rate |
|-----------|----------|-------------------|-------------|------------|
| In-Memory | ~9,200   | 11 ms             | 48 ms       | <0.1%      |
| Redis     | ~2,800   | 18 ms             | 70 ms       | ~0.5%      |

ğŸ“ˆ Latency vs RPS graph:  
![Benchmark Results](benchmarks/report_plot.png)

ğŸ“‰ [Download raw benchmark data](benchmarks/sample_report.csv)
```
---

## ğŸ—ï¸ Architecture

The system is modular and deliberately simple:

Client â†’ HTTP Middleware â†’ TokenBucket â†’ Store (MemStore / Redis)

![Architecture Diagram](docs/diagram.png)


---

## ğŸ’» Usage as a Library

```go
import "github.com/ChiragBaid/rate-limiter/internal/limiter"

store := backend.NewMemStore()
tb := limiter.NewTokenBucket(store, 10, 5) // 10 tokens/sec, burst 5

allowed := tb.Allow("user123")
if allowed {
    fmt.Println("Request allowed")
} else {
    fmt.Println("Rate limit exceeded")
}
```
---

```markdown
## âš–ï¸ Design Tradeoffs
- **Token-bucket vs Sliding-window** â†’ token-bucket allows bursts; sliding-window is stricter.  
- **Distributed mode** â†’ Redis backend here is simplified (HGET/HSET). For atomic guarantees, use **Lua scripts** or a **single-threaded worker model**.  
- **Performance** â†’ in-memory mode can sustain ~9k RPS on commodity hardware. Redis adds network + serialization overhead.  
- **Extensibility** â†’ `Store` interface can support other backends (e.g., PostgreSQL, DynamoDB, etc.).
```
---

## ğŸ”® Future Work
- Add **sliding-window** algorithm support.  
- Add **Prometheus metrics** and `/metrics` endpoint.  
- Provide **gRPC interface** in addition to HTTP middleware.  
- Explore **Kubernetes deployment manifests**.  

---

## âœ… Attribution
This repository is a **fresh implementation** for educational + interview prep.  
Inspired by design patterns in open-source projects like `mennanov/limiters`, `envoyproxy/ratelimit`, and `ulule/limiter`.

---
